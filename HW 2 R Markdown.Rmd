---
title: "Homework 2"
author: "Asrorbek Orzikulov, Sofya Simakova"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, message=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.align = 'center', echo = TRUE)
```

# 1 Comparison of estimators (5pts)

## a.
We will first write a likelihood function for $X \sim {Bin}(n, \theta)$.
$$\mathcal{L} \left(\theta, X\right)= P_{X}\left(x\right)=\left(\begin{array}{c}
n\\
X
\end{array}\right) \theta^{X}(1-\theta)^{(n-X)}$$
$$\ell \left(\theta, X \right)=\log \mathcal{L} \left(\theta, X\right)=\log \left(\begin{array}{c}
n\\
X
\end{array}\right)
+ X \log \theta+\left(n-X\right) \log (1-\theta)$$
$$\frac{\partial \ell \left(\theta, X\right)}{\partial \theta}=\frac{X}{\theta}-\frac{n-X}{1-\theta}=0$$
$$\frac{X}{\theta}= \frac{ n-X}{1-\theta}$$
$${X}-{X} \theta= n \theta -{X} \theta$$
$${X}=\theta n$$
$$\boxed{\hat{\theta}^{ML}=\frac{X}{n}}$$
To check if this value is indeed maximum, we should consider the second derivative of the log-likelihood function.
$$\frac{\partial^2 \ell \left(\theta, X\right)}{\partial \theta^2} = - \frac{X}{\theta^2} - \frac{n-X}{\left(1-\theta\right)^2} < 0$$
Also, we should demonstrate that the optimum is not at the boundaries of the interval for $\theta$ -- $[0, 1]$. Since $\ell \left(\theta, X\right)$ depends on $\log \theta$ and $\log (1 - \theta)$, the value of the log-likelihood function when $\theta$ is either $0$ or $1$ is $-\infty$. Therefore, we can conclude that $\hat{\theta}^{ML}$ is a Maximum Likelihood Estimator.
$$\mathbb E \; \hat{\theta}^{ML}= \mathbb E \left[\frac{X}{n}\right] =\frac{1}{n} \mathbb EX =\frac{1}{n} n \theta = \theta$$
From the fact that the mean of a binomial distribution is $n\theta$ and its variance is $n\theta(1-\theta)$ we have:
$$Var\:\hat{\theta}^{ML}= Var\left[\frac{X}{n}\right]=\frac{1}{n^2} Var X =\frac{1}{n^2} n\theta(1-\theta)=\frac{\theta(1-\theta)}{n}$$

## b.
$$\hat{\theta}^{alt.}=\frac{(X+1)}{(n+2)}$$
$$\mathbb E \:\hat{\theta}^{alt.} = \mathbb E\left[\frac{X+1}{n+2}\right] = \frac{1}{n+2} \mathbb E[X+1]=\frac{1}{n+2}[n \theta+1]=\frac{n \theta+1}{n+2}$$
$$Var\:\hat{\theta}^{alt.}=Var\left[\frac{X+1}{n+2}\right]=\frac{1}{(n+2)^{2}} \operatorname{Var}[X+1]=\frac{\operatorname{Var}[X]}{(n+2)^{2}}=\frac{n \theta(1-\theta)}{(n+2)^{2}}$$

## c.
$$\operatorname{MSE}\left(\hat{\theta}^{ML}\right)=\left(E\hat{\theta}^{ML}-\theta\right)^{2}+\operatorname{Var} \hat{\theta}^{ML}=(\theta-\theta)^{2}+\frac{\theta(1-\theta)}{n}=\frac{ \theta(1-\theta)}{n}$$
\begin{align*}
\operatorname{MSE} \left( \hat{\theta}^{alt} \right) &= \left(E \hat{\theta}^{alt} - \theta\right)^{2}+\operatorname{Var} \hat{\theta}^{alt} = \left(\frac{n \theta+1} {n+2} -\theta\right)^{2} + \frac{n \theta(1-\theta)} {(n+2)^{2}}\\
&= \left(\frac{n\theta+1-n \theta - 2 \theta}{n+2}\right)^{2} +\frac{n \theta(1-\theta)}{(n+2)^{2}} = \frac{(1-2 \theta)^{2} + n \theta(1-\theta)}{(n+2)^{2}}\\
&= \frac{1-4 \theta+4 \theta^{2}+n \theta-n \theta^{2}}{(n+2)^{2}} = \frac{1+(n-4) \theta-(n-4) \theta^{2}}{(n+2)^{2}}
\end{align*}

## d.
Simulate a sample from a binomial distribution for some fixed $\theta$ and n.

```{r}
rm(list = ls())
n <- 50
theta <- 0.73
X <- rbinom(n = 1, size = n, prob = theta)
X
```

Calculate the two estimators, their bias and their variance
```{r}
theta_mle <- X / n
bias_mle <- 0
var_mle <- theta * (1-theta) / n
mse_mle <- bias_mle^2 + var_mle

theta_alt <- (X+1) / (n+2)
bias_alt <- (n*theta + 1) / (n+2) - theta
var_alt <- (n*theta*(1-theta)) / (n+2)^2
mse_alt <- bias_alt^2 + var_alt

paste("theta_mle is", theta_mle)
paste("bias_mle is", bias_mle)
paste("var_mle is", var_mle)
paste("mse_mle is", mse_mle)

paste("theta_alt is", theta_alt)
paste("bias_alt is", bias_alt)
paste("var_alt is", var_alt)
paste("mse_alt is", mse_alt)
```

Represent (on the same graph), the MSE of $\theta^{MLE}$ and $\theta^{alt.}$ as a function of
$\theta \in$	 [0; 1] for fixed n.
```{r}
thetas <- seq(0, 1, by = 0.005)
mse_mle <- c()
mse_alt <- c()
for (theta in thetas) {
  mse_mle <- c(mse_mle, theta * (1-theta) / n)
  mse_alt <- c(mse_alt, (1 + (n-4)*theta - (n-4)*theta^2) / (n+2)^2)
}

plot(thetas, mse_mle, type = 'l', col = 'red', lwd = 2, ylab = "MSE")
points(thetas, mse_alt, type = 'l', col = 'blue', lwd = 2, ylab = "MSE")

legend("topleft", legend = c("MLE", "Alt."),
       col = c("red", "blue"), lty = 1, cex = 1)
```

# 4 Data analysis (5pts).

## a.
```{r}
library(tidyverse)
mydata <- read_csv("poulpeF.csv")
summary(mydata)
```

We see from looking at the summary that the lowest octopus weight represented is 40 and the highest is 2400. The median (545) is lower than the mean (639) - meaning, distribution ow weights is right-skewed. 
## b.

Calculating MLE of the mean:

```{r}
sum <- 0
n <-0
for (x in mydata$Poids){
  sum <- sum + x
  n <- n+1
}
mle_mean = sum/n
paste("MLE of the mean is ", mle_mean)
```

Calculating MLE of the variance:

```{r}
sum <- 0
n <- 0
mean <- mle_mean #we know that for normal distribution, mle of mean is equal to sample mean

for (x in mydata$Poids){
  diff_squared <- (x - mean)^2
  sum <- sum + diff_squared
  n <- n+1
}
mle_var <- sum / n
paste("MLE of the variance is ", mle_var)
```

## c.
```{r}
hist(mydata$Poids)
```

From the graph it is clear that the distribution of weights does not follow the Gaussian model.
Is skewes heavily to the right. Using Gaussian model for calculations would be a problem, 
as it would not provide accurate results.

# d.
```{r}
variance <- mle_var # we can say that because this is a Gaussian distribution
lower <- mean - 1.95 * sqrt(variance) / sqrt(n)
paste("Lower bound of the confidence interval is ", lower)
```
```{r}
upper <- mean + 1.95 * sqrt(variance) / sqrt(n)
paste("Upper bound of the confidence interval is ", upper)
```