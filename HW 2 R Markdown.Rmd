---
title: "Homework 2"
author: "Asrorbek Orzikulov, Sofya Simakova"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, message=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.align = 'center', echo = TRUE)
```

# 1 Comparison of estimators (5pts)

## Part a
We will first write a likelihood function for $X \sim {Bin}(n, \theta)$.
$$\mathcal{L} \left(\theta, X\right)= P_{X}\left(x\right)=\left(\begin{array}{c}
n\\
X
\end{array}\right) \theta^{X}(1-\theta)^{(n-X)}$$
$$\ell \left(\theta, X \right)=\log \mathcal{L} \left(\theta, X\right)=\log \left(\begin{array}{c}
n\\
X
\end{array}\right)
+ X \log \theta+\left(n-X\right) \log (1-\theta)$$
$$\frac{\partial \ell \left(\theta, X\right)}{\partial \theta}=\frac{X}{\theta}-\frac{n-X}{1-\theta}=0$$
$$\frac{X}{\theta}= \frac{ n-X}{1-\theta}$$
$${X}-{X} \theta= n \theta -{X} \theta$$
$${X}=\theta n$$
$$\boxed{\hat{\theta}^{ML}=\frac{X}{n}}$$
To check if this value is indeed maximum, we should consider the second derivative of the log-likelihood function.
$$\frac{\partial^2 \ell \left(\theta, X\right)}{\partial \theta^2} = - \frac{X}{\theta^2} - \frac{n-X}{\left(1-\theta\right)^2} < 0$$
Also, we should demonstrate that the optimum is not at the boundaries of the interval for $\theta$ -- $[0, 1]$. Since $\ell \left(\theta, X\right)$ depends on $\log \theta$ and $\log (1 - \theta)$, the value of the log-likelihood function when $\theta$ is either $0$ or $1$ is $-\infty$. Therefore, we can conclude that $\hat{\theta}^{ML}$ is a Maximum Likelihood Estimator.

From the fact that the mean of a binomial distribution is $n\theta$ and its variance is $n\theta(1-\theta)$ we have:
$$\mathbb E \; \hat{\theta}^{ML}= \mathbb E \left[\frac{X}{n}\right] =\frac{1}{n} \mathbb EX =\frac{1}{n} n \theta = \theta$$
$$Var\:\hat{\theta}^{ML}= Var\left[\frac{X}{n}\right]=\frac{1}{n^2} Var X =\frac{1}{n^2} n\theta(1-\theta)=\frac{\theta(1-\theta)}{n}$$

## Part b
$$\hat{\theta}^{alt}=\frac{X+1}{n+2}$$
$$\mathbb E \:\hat{\theta}^{alt} = \mathbb E\left[\frac{X+1}{n+2}\right] = \frac{1}{n+2} \mathbb E[X+1]=\frac{1}{n+2}[n \theta+1]=\frac{n \theta+1}{n+2}$$
$$Var\:\hat{\theta}^{alt}=Var\left[\frac{X+1}{n+2}\right]=\frac{1}{(n+2)^{2}} \operatorname{Var}[X+1]=\frac{\operatorname{Var}[X]}{(n+2)^{2}}=\frac{n \theta(1-\theta)}{(n+2)^{2}}$$

## Part c
$$\operatorname{Bias} {\hat{\theta}^{ML}} = \mathbb E \hat{\theta}^{ML} - \theta = \theta - \theta = 0$$
$$Var \; {\hat{\theta}^{ML}} = \frac{ \theta(1-\theta)}{n}$$
$$\operatorname{MSE} \; \hat{\theta}^{ML} = \left( \operatorname{Bias} {\hat{\theta}^{ML}} \right)^2 + Var \; {\hat{\theta}^{ML}} = 0^2 + \frac{\theta(1-\theta)}{n} = \frac{ \theta(1-\theta)}{n}$$

$$\operatorname{Bias} {\hat{\theta}^{alt}} = \mathbb E \hat{\theta}^{alt} - \theta = \frac{n \theta+1} {n+2} -\theta = \frac{n\theta+1-n \theta - 2 \theta}{n+2} = \frac{1 - 2 \theta}{n+2}$$
$$Var\:\hat{\theta}^{alt} = \frac{n \theta(1-\theta)}{(n+2)^{2}}$$
\begin{align*}
\operatorname{MSE} \; \hat{\theta}^{alt} &= \left( \operatorname{Bias} {\hat{\theta}^{alt}} \right)^2 + Var \; {\hat{\theta}^{alt}} = \left(\frac{1 - 2 \theta}{n+2} \right)^2 + \frac{n \theta(1-\theta)} {(n+2)^{2}}\\
&= \frac{(1-2 \theta)^{2} + n \theta(1-\theta)}{(n+2)^{2}} = \frac{1-4 \theta+4 \theta^{2}+n \theta-n \theta^{2}}{(n+2)^{2}}\\
&= \frac{1+(n-4) \theta-(n-4) \theta^{2}}{(n+2)^{2}}
\end{align*}

Comparison of the two estimators leads us to 4 immediate conclusions:

  1. The MLE is unbiased, while the alternative estimator is biased whenever $\theta \neq 0.5$.
  2. Since $\frac{1}{n} > \frac{n}{(n+2)^2}$, the variance of the alternative estimator is smaller than that of the MLE. As a result, we can observe so-called bias-variance tradeoff in this question.
  3. As $n\to\infty$, the bias of $\hat{\theta}^{alt}$ goes to zero and the difference between  $\frac{1}{n}$ and $\frac{n}{(n+2)^2}$ diminishes. Therefore, when $n$ is large, these two estimators are almost identical.
  4. If one wants to carry out a statistical analysis (e.g. conduct hypothesis testing or construct a confidence interval), they should choose the MLE since it is unbiased. However, we will see in **part d** that the MSE of $\hat{\theta}^{alt}$ is lower than that of $\hat{\theta}^{ML}$ when $\theta \in [0.15, 0.85]$. Over this range, data scientists would prefer $\hat{\theta}^{alt}$ to $\hat{\theta}^{ML}$ because for any single instance of true $\theta$, the alternative estimate will be closer to the actual value.

## Part d
For this question, we will use $n = 50$ and $\theta = 0.73$.
```{r}
rm(list = ls())
n <- 50
theta <- 0.73
X <- rbinom(n = 1, size = n, prob = theta)
X
```

```{r}
theta_mle <- X / n
bias_mle <- 0
var_mle <- theta * (1-theta) / n
mse_mle <- bias_mle^2 + var_mle

theta_alt <- (X+1) / (n+2)
bias_alt <- (1 - 2*theta) / (n+2)
var_alt <- (n*theta*(1-theta)) / (n+2)^2
mse_alt <- bias_alt^2 + var_alt

paste("theta_mle is", theta_mle)
paste("bias_mle is", bias_mle)
paste("var_mle is", var_mle)
paste("mse_mle is", mse_mle)

paste("theta_alt is", theta_alt)
paste("bias_alt is", bias_alt)
paste("var_alt is", var_alt)
paste("mse_alt is", mse_alt)
```

For this question, we will use $n = 50$ and show the MSE of $\hat \theta^{MLE}$ and $\hat \theta^{alt.}$ as a function of $\theta \in [0, 1]$.
```{r}
thetas <- seq(0, 1, by = 0.005)
mse_mle <- c()
mse_alt <- c()
for (theta in thetas) {
  mse_mle <- c(mse_mle, theta * (1-theta) / n)
  mse_alt <- c(mse_alt, (1 + (n-4)*theta - (n-4)*theta^2) / (n+2)^2)
}

plot(thetas, mse_mle, type = 'l', col = 'red', lwd = 2, ylab = "MSE")
points(thetas, mse_alt, type = 'l', col = 'blue', lwd = 2, ylab = "MSE")

legend("topleft", legend = c("MLE", "alternative"),
       col = c("red", "blue"), lty = 1, cex = 1)
```

# 4 Data analysis (5pts).
## Part a
```{r}
mydata <- read_csv("poulpeF.csv")
summary(mydata)
```
We see from looking at the summary that the lowest octopus weight represented is 40 and the highest is 2400. The median (545) is lower than the mean (639), suggesting that the distribution of weights is right-skewed. 

## Part b
We saw in class that the MLE of the mean and variance of a normal distribution are the empirical mean and empirical variance of data. In R, these can be conveniently calculated using `mean` and `var` functions. Since the formula for the built-in `var` function uses $n-1$ in its denominator, we will just scale it appropriately.
```{r}
sum <- 0
n <-0
for (x in mydata$Poids){
  sum <- sum + x
  n <- n+1
}
mle_mean = sum/n
paste("MLE of the mean is ", mle_mean)
paste("MLE of the mean is ", mean(mydata$Poids))
```

```{r}
sum <- 0
n <- 0
mean <- mle_mean

for (x in mydata$Poids){
  diff_squared <- (x - mean)^2
  sum <- sum + diff_squared
  n <- n+1
}
mle_var <- sum / n
paste("MLE of the variance is ", mle_var)
paste("MLE of the variance is ", var(mydata$Poids) * (n-1)/n)
```

## c.
```{r}
hist(mydata$Poids)
```

From the graph it is clear that the distribution of weights does not follow the Gaussian model.
Is skewes heavily to the right. Using Gaussian model for calculations would be a problem, 
as it would not provide accurate results.

# d.
```{r}
variance <- mle_var # we can say that because this is a Gaussian distribution
lower <- mean - 1.95 * sqrt(variance) / sqrt(n)
paste("Lower bound of the confidence interval is ", lower)
```
```{r}
upper <- mean + 1.95 * sqrt(variance) / sqrt(n)
paste("Upper bound of the confidence interval is ", upper)
```