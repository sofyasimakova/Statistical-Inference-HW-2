---
title: "Statistical Inference: Homework 2"
author: "Asrorbek Orzikulov, Sofya Simakova"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, message=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.align = 'center', echo = TRUE)
```

# 1 Comparison of estimators

## Part a
We will first write a likelihood function for $X \sim {Bin}(n, \theta)$.
$$\mathcal{L} \left(\theta, X\right)= P_{X}\left(x\right)=\left(\begin{array}{c}
n\\
X
\end{array}\right) \theta^{X}(1-\theta)^{(n-X)}$$
$$\ell \left(\theta, X \right)=\log \mathcal{L} \left(\theta, X\right)=\log \left(\begin{array}{c}
n\\
X
\end{array}\right)
+ X \log \theta+\left(n-X\right) \log (1-\theta)$$
$$\frac{\partial \ell \left(\theta, X\right)}{\partial \theta}=\frac{X}{\theta}-\frac{n-X}{1-\theta}=0$$
$$\frac{X}{\theta}= \frac{ n-X}{1-\theta}$$
$${X}-{X} \theta= n \theta -{X} \theta$$
$${X}=\theta n$$
$$\boxed{\hat{\theta}^{ML}=\frac{X}{n}}$$
To check if this value is indeed maximum, we should consider the second derivative of the log-likelihood function.
$$\frac{\partial^2 \ell \left(\theta, X\right)}{\partial \theta^2} = - \frac{X}{\theta^2} - \frac{n-X}{\left(1-\theta\right)^2} < 0$$
Also, we should demonstrate that the optimum is not at the boundaries of the interval for $\theta$ -- $[0, 1]$. Since $\ell \left(\theta, X\right)$ depends on $\log \theta$ and $\log (1 - \theta)$, the value of the log-likelihood function when $\theta$ is either $0$ or $1$ is $-\infty$. Therefore, we can conclude that $\hat{\theta}^{ML}$ is a Maximum Likelihood Estimator.

From the fact that the mean of a binomial distribution is $n\theta$ and its variance is $n\theta(1-\theta)$ we have:
$$\mathbb E \; \hat{\theta}^{ML}= \mathbb E \left[\frac{X}{n}\right] =\frac{1}{n} \mathbb EX =\frac{1}{n} n \theta = \theta$$
$$Var\:\hat{\theta}^{ML}= Var\left[\frac{X}{n}\right]=\frac{1}{n^2} Var X =\frac{1}{n^2} n\theta(1-\theta)=\frac{\theta(1-\theta)}{n}$$

## Part b
$$\hat{\theta}^{alt}=\frac{X+1}{n+2}$$
$$\mathbb E \:\hat{\theta}^{alt} = \mathbb E\left[\frac{X+1}{n+2}\right] = \frac{1}{n+2} \mathbb E[X+1]=\frac{1}{n+2}[n \theta+1]=\frac{n \theta+1}{n+2}$$
$$Var\:\hat{\theta}^{alt}=Var\left[\frac{X+1}{n+2}\right]=\frac{1}{(n+2)^{2}} \operatorname{Var}[X+1]=\frac{\operatorname{Var}[X]}{(n+2)^{2}}=\frac{n \theta(1-\theta)}{(n+2)^{2}}$$

## Part c
$$\operatorname{Bias} {\hat{\theta}^{ML}} = \mathbb E \hat{\theta}^{ML} - \theta = \theta - \theta = 0$$
$$Var \; {\hat{\theta}^{ML}} = \frac{ \theta(1-\theta)}{n}$$
$$\operatorname{MSE} \; \hat{\theta}^{ML} = \left( \operatorname{Bias} {\hat{\theta}^{ML}} \right)^2 + Var \; {\hat{\theta}^{ML}} = 0^2 + \frac{\theta(1-\theta)}{n} = \frac{ \theta(1-\theta)}{n}$$

$$\operatorname{Bias} {\hat{\theta}^{alt}} = \mathbb E \hat{\theta}^{alt} - \theta = \frac{n \theta+1} {n+2} -\theta = \frac{n\theta+1-n \theta - 2 \theta}{n+2} = \frac{1 - 2 \theta}{n+2}$$
$$Var\:\hat{\theta}^{alt} = \frac{n \theta(1-\theta)}{(n+2)^{2}}$$
\begin{align*}
\operatorname{MSE} \; \hat{\theta}^{alt} &= \left( \operatorname{Bias} {\hat{\theta}^{alt}} \right)^2 + Var \; {\hat{\theta}^{alt}} = \left(\frac{1 - 2 \theta}{n+2} \right)^2 + \frac{n \theta(1-\theta)} {(n+2)^{2}}\\
&= \frac{(1-2 \theta)^{2} + n \theta(1-\theta)}{(n+2)^{2}} = \frac{1-4 \theta+4 \theta^{2}+n \theta-n \theta^{2}}{(n+2)^{2}}\\
&= \frac{1+(n-4) \theta-(n-4) \theta^{2}}{(n+2)^{2}}
\end{align*}

Comparison of the two estimators leads us to 4 immediate conclusions:

  1. The MLE is unbiased, while the alternative estimator is biased whenever $\theta \neq 0.5$.
  2. Since $\frac{1}{n} > \frac{n}{(n+2)^2}$, the variance of the alternative estimator is smaller than that of the MLE. As a result, we can observe so-called bias-variance tradeoff in this question.
  3. As $n\to\infty$, the bias of $\hat{\theta}^{alt}$ goes to zero and the difference between  $\frac{1}{n}$ and $\frac{n}{(n+2)^2}$ diminishes. Therefore, when $n$ is large, these two estimators are almost identical.
  4. If one wants to carry out a statistical analysis (e.g. conduct hypothesis testing or construct a confidence interval), they should choose the MLE since it is unbiased. However, we will see in **part d** that the MSE of $\hat{\theta}^{alt}$ is lower than that of $\hat{\theta}^{ML}$ when $\theta \in [0.15, 0.85]$. Over this range, data scientists would prefer $\hat{\theta}^{alt}$ to $\hat{\theta}^{ML}$ because for any single instance of true $\theta$, the alternative estimate will be closer to the actual value.

## Part d
For this question, we will use $n = 50$ and $\theta = 0.73$.
```{r}
rm(list = ls())
n <- 50
theta <- 0.73
X <- rbinom(n = 1, size = n, prob = theta)
X
```

```{r}
theta_mle <- X / n
bias_mle <- 0
var_mle <- theta * (1-theta) / n
mse_mle <- bias_mle^2 + var_mle
print(c("theta_mle" = theta_mle, "bias_mle" = bias_mle, 
        "var_mle" = var_mle, "mse_mle" = mse_mle))

theta_alt <- (X+1) / (n+2)
bias_alt <- (1 - 2*theta) / (n+2)
var_alt <- (n*theta*(1-theta)) / (n+2)^2
mse_alt <- bias_alt^2 + var_alt
print(c("theta_alt" = theta_alt, "bias_alt" = bias_alt, 
        "var_alt" = var_alt, "mse_alt" = mse_alt))
```

For this question, we will use $n = 50$ and show the MSE of $\hat \theta^{MLE}$ and $\hat \theta^{alt}$ as a function of $\theta \in [0, 1]$.
```{r}
thetas <- seq(0, 1, by = 0.005)
mse_mle <- c()
mse_alt <- c()
for (theta in thetas) {
  mse_mle <- c(mse_mle, theta * (1-theta) / n)
  mse_alt <- c(mse_alt, (1 + (n-4)*theta - (n-4)*theta^2) / (n+2)^2)
}
plot(thetas, mse_mle, type = 'l', col = 'red', lwd = 2, ylab = "MSE")
points(thetas, mse_alt, type = 'l', col = 'blue', lwd = 2, ylab = "MSE")
legend("topleft", legend = c("MLE", "alternative"),
       col = c("red", "blue"), lty = 1, cex = 1)
```

# 4 Data analysis (5pts).
## Part a
```{r, message=FALSE}
rm(list = ls())
mydata <- read_csv("poulpeF.csv")
summary(mydata)
```
We see from looking at the summary that the lowest octopus weight represented is 40 and the highest is 2400. The median (545) is lower than the mean (639), suggesting that the distribution of weights is right-skewed. 

## Part b
We saw in the class that the MLE of the mean and variance of a normal distribution are the empirical mean and empirical variance of data. In R, these can be conveniently calculated using `mean` and `var` functions. Since the formula for the built-in `var` function uses $n-1$ in its denominator, we will just scale it appropriately.
```{r}
n <- length(mydata$Poids)
mle_mean <- mean(mydata$Poids)
mle_var <- var(mydata$Poids) * (n-1)/n
print(c("MLE of the mean" = mle_mean, "     MLE of the variance" = mle_var))
```

## Part c
```{r}
hist(mydata$Poids, main = "Distribution of Data", xlab = "Weigths")
```
From the graph, it is clear that the distribution of weights does not follow a Gaussian model. Using a Gaussian model for hypothesis testing or interval estimation would be a problem because of two reasons:

  1. The distribution is heavily skewed to the right.
  2. The distribution is bounded below by zero, and the proportion of observations in the leftmost bin is not small.

Therefore, a Gaussian model does not seem appropriate for the above distribution. 

# d.
We will use $\bar X_n$ for the empirical mean and $\hat \sigma$ for the empirical variance, both of which are MLE for a normal distribution. From the Central Limit Theorem, we know the following:
$$\frac {\sqrt{n} \left(\bar X_n - \mu\right)}{\sigma} \xrightarrow{D} \mathcal{N}(0, 1)$$
In addition, a normal distribution satisfies the regularity conditions covered in the class. Therefore, the MLE estimator of the population variance, $\hat \sigma$, is consistent estimator of $\sigma$. Said otherwise, $\hat \sigma \xrightarrow{P} \sigma$. Using the Continuous Map Theorem several times as well as the Slutsky's Theorem, we will arrive at the following result.
$$\frac {\sqrt{n} \left(\bar X_n - \mu\right)}{\sigma} \frac{\sigma}{\hat \sigma} \xrightarrow{D} \mathcal{N}(0, 1) \times 1$$
$$\frac {\sqrt{n} \left(\bar X_n - \mu\right)}{\hat \sigma} \xrightarrow{D} \mathcal{N}(0, 1)$$
Putting it all together, we will arrive at a confidence interval for $\mu$:
$$\bar X_n \pm z_{1 - \alpha / 2} \times \frac{\hat \sigma}{\sqrt{n}}$$
```{r}
alpha <- 0.05
z_score <- qnorm(1 - alpha/2, 0, 1, TRUE)
lower <- mle_mean - z_score * sqrt(mle_var) / sqrt(n)
upper <- mle_mean + z_score * sqrt(mle_var) / sqrt(n)
paste("Lower bound of the confidence interval is", lower)
paste("Upper bound of the confidence interval is", upper)
```