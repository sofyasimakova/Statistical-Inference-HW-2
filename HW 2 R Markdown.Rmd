---
title: "Statistical Inference: Homework 2"
author: "Asrorbek Orzikulov, Sofya Simakova"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, message=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.align = 'center', echo = TRUE)
```

# 1 Comparison of estimators

## Part a
We will first write a likelihood function for $X \sim {Bin}(n, \theta)$.
$$\mathcal{L} \left(\theta, X\right)= P_{X}\left(x\right)=\left(\begin{array}{c}
n\\
X
\end{array}\right) \theta^{X}(1-\theta)^{(n-X)}$$
$$\ell \left(\theta, X \right)=\log \mathcal{L} \left(\theta, X\right)=\log \left(\begin{array}{c}
n\\
X
\end{array}\right)
+ X \log \theta+\left(n-X\right) \log (1-\theta)$$
$$\frac{\partial \ell \left(\theta, X\right)}{\partial \theta}=\frac{X}{\theta}-\frac{n-X}{1-\theta}=0$$
$$\frac{X}{\theta}= \frac{ n-X}{1-\theta}$$
$${X}-{X} \theta= n \theta -{X} \theta$$
$${X}=\theta n$$
$$\boxed{\hat{\theta}^{ML}=\frac{X}{n}}$$
To check if this value is indeed maximum, we should consider the second derivative of the log-likelihood function.
$$\frac{\partial^2 \ell \left(\theta, X\right)}{\partial \theta^2} = - \frac{X}{\theta^2} - \frac{n-X}{\left(1-\theta\right)^2} < 0$$
Also, we should demonstrate that the optimum is not at the boundaries of the interval for $\theta$ -- $[0, 1]$. Since $\ell \left(\theta, X\right)$ depends on $\log \theta$ and $\log (1 - \theta)$, the value of the log-likelihood function when $\theta$ is either $0$ or $1$ is $-\infty$. Therefore, we can conclude that $\hat{\theta}^{ML}$ is a Maximum Likelihood Estimator.

From the fact that the mean of a binomial distribution is $n\theta$ and its variance is $n\theta(1-\theta)$ we have:
$$\mathbb E \; \hat{\theta}^{ML}= \mathbb E \left[\frac{X}{n}\right] =\frac{1}{n} \mathbb EX =\frac{1}{n} n \theta = \theta$$
$$Var\:\hat{\theta}^{ML}= Var\left[\frac{X}{n}\right]=\frac{1}{n^2} Var X =\frac{1}{n^2} n\theta(1-\theta)=\frac{\theta(1-\theta)}{n}$$

## Part b
$$\hat{\theta}^{alt}=\frac{X+1}{n+2}$$
$$\mathbb E \:\hat{\theta}^{alt} = \mathbb E\left[\frac{X+1}{n+2}\right] = \frac{1}{n+2} \mathbb E[X+1]=\frac{1}{n+2}[n \theta+1]=\frac{n \theta+1}{n+2}$$
$$Var\:\hat{\theta}^{alt}=Var\left[\frac{X+1}{n+2}\right]=\frac{1}{(n+2)^{2}} \operatorname{Var}[X+1]=\frac{\operatorname{Var}[X]}{(n+2)^{2}}=\frac{n \theta(1-\theta)}{(n+2)^{2}}$$

## Part c
$$\operatorname{Bias} {\hat{\theta}^{ML}} = \mathbb E \hat{\theta}^{ML} - \theta = \theta - \theta = 0$$
$$Var \; {\hat{\theta}^{ML}} = \frac{ \theta(1-\theta)}{n}$$
$$\operatorname{MSE} \; \hat{\theta}^{ML} = \left( \operatorname{Bias} {\hat{\theta}^{ML}} \right)^2 + Var \; {\hat{\theta}^{ML}} = 0^2 + \frac{\theta(1-\theta)}{n} = \frac{ \theta(1-\theta)}{n}$$

$$\operatorname{Bias} {\hat{\theta}^{alt}} = \mathbb E \hat{\theta}^{alt} - \theta = \frac{n \theta+1} {n+2} -\theta = \frac{n\theta+1-n \theta - 2 \theta}{n+2} = \frac{1 - 2 \theta}{n+2}$$
$$Var\:\hat{\theta}^{alt} = \frac{n \theta(1-\theta)}{(n+2)^{2}}$$
\begin{align*}
\operatorname{MSE} \; \hat{\theta}^{alt} &= \left( \operatorname{Bias} {\hat{\theta}^{alt}} \right)^2 + Var \; {\hat{\theta}^{alt}} = \left(\frac{1 - 2 \theta}{n+2} \right)^2 + \frac{n \theta(1-\theta)} {(n+2)^{2}}\\
&= \frac{(1-2 \theta)^{2} + n \theta(1-\theta)}{(n+2)^{2}} = \frac{1-4 \theta+4 \theta^{2}+n \theta-n \theta^{2}}{(n+2)^{2}}\\
&= \frac{1+(n-4) \theta-(n-4) \theta^{2}}{(n+2)^{2}}
\end{align*}

Comparison of the two estimators leads us to 4 immediate conclusions:

  1. The MLE is unbiased, while the alternative estimator is biased whenever $\theta \neq 0.5$.
  2. Since $\frac{1}{n} > \frac{n}{(n+2)^2}$, the variance of the alternative estimator is smaller than that of the MLE. As a result, we can observe so-called bias-variance tradeoff in this question.
  3. As $n\to\infty$, the bias of $\hat{\theta}^{alt}$ goes to zero and the difference between  $\frac{1}{n}$ and $\frac{n}{(n+2)^2}$ diminishes. Therefore, when $n$ is large, these two estimators are almost identical.
  4. If one wants to carry out a statistical analysis (e.g. conduct hypothesis testing or construct a confidence interval), they should choose the MLE since it is unbiased. However, we will see in **part d** that the MSE of $\hat{\theta}^{alt}$ is lower than that of $\hat{\theta}^{ML}$ when $\theta \in [0.15, 0.85]$. Over this range, data scientists would prefer $\hat{\theta}^{alt}$ to $\hat{\theta}^{ML}$ because for any single instance of true $\theta$, the alternative estimate will be closer to the actual value.

## Part d
For this question, we will use $n = 50$ and $\theta = 0.73$.
```{r}
rm(list = ls())
n <- 50
theta <- 0.73
X <- rbinom(n = 1, size = n, prob = theta)
X
```

```{r}
theta_mle <- X / n
bias_mle <- 0
var_mle <- theta * (1-theta) / n
mse_mle <- bias_mle^2 + var_mle
print(c("theta_mle" = theta_mle, "bias_mle" = bias_mle, 
        "var_mle" = var_mle, "mse_mle" = mse_mle))

theta_alt <- (X+1) / (n+2)
bias_alt <- (1 - 2*theta) / (n+2)
var_alt <- (n*theta*(1-theta)) / (n+2)^2
mse_alt <- bias_alt^2 + var_alt
print(c("theta_alt" = theta_alt, "bias_alt" = bias_alt, 
        "var_alt" = var_alt, "mse_alt" = mse_alt))
```

For this question, we will use $n = 50$ and show the MSE of $\hat \theta^{MLE}$ and $\hat \theta^{alt}$ as a function of $\theta \in [0, 1]$.
```{r}
thetas <- seq(0, 1, by = 0.005)
mse_mle <- c()
mse_alt <- c()
for (theta in thetas) {
  mse_mle <- c(mse_mle, theta * (1-theta) / n)
  mse_alt <- c(mse_alt, (1 + (n-4)*theta - (n-4)*theta^2) / (n+2)^2)
}
plot(thetas, mse_mle, type = 'l', col = 'red', lwd = 2, ylab = "MSE")
points(thetas, mse_alt, type = 'l', col = 'blue', lwd = 2, ylab = "MSE")
legend("topleft", legend = c("MLE", "alternative"),
       col = c("red", "blue"), lty = 1, cex = 1)
```

# 2 Robustness of the estimators
## Part a
The maximum likelihood function for a Gaussian model whose variance is 1 is given below.
$$\mathcal {L}_n(\theta,X_1, \ldots, X_n) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (X_i -\theta)^{2}}$$
To compute the Maximum Likelihood estimator $\hat{\theta}^{ML}$, we find the zero of the first derivative of the log-likelihood function.
\begin{align*}
\ell_n (\theta,X_1, \ldots , X_n) &= \log \mathcal {L}_n(\theta,X_1, \ldots, X_n)= \log \left(\prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} (X_i -\theta)^{2}} \right)\\
&= \sum_{i=1}^{n}\log \left(\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} (X_i -\theta)^{2}} \right) = \sum_{i=1}^{n} \left(\log \frac{1}{\sqrt{2\pi}} + \log  e^{-\frac{1}{2} (X_i -\theta)^{2}} \right)\\
&= \sum_{i=1}^{n} \left(\log \frac{1}{\sqrt{2\pi}} -\frac{1}{2} (X_i -\theta)^{2} \right) = n \log \frac{1}{\sqrt{2\pi}} -\frac{1}{2} \sum_{i=1}^{n} \left(X_i -\theta \right)^{2}
\end{align*}
$$\frac{\partial \ell_n (\theta,X_1, \ldots , X_n)}{\partial \theta} = -\frac{1}{2} \sum_{i=1}^{n} 2(X_i - \theta) (-1) = 0$$
$$\frac{\partial \ell_n (\theta,X_1, \ldots , X_n)}{\partial \theta} = \sum_{i=1}^{n} (X_i - \theta) = 0$$
$$\sum_{i=1}^{n}(X_i - \theta) = 0$$
$$\sum_{i=1}^{n} X_i - \sum_{i=1}^{n} \theta = 0$$
$$n\theta= \sum_{i=1}^{n} X_i$$
$$\boxed{\hat{\theta}^{ML} = \bar{X}_n}$$
To prove that this estimator is indeed maximum, we will demonstrate that the second derivative is negative.
$$\frac{\partial^2 \ell_n (\theta,X_1, \ldots , X_n)}{\partial \theta^2} = \sum_{i=1}^{n} (- 1) = -n < 0$$
Using this $\hat{\theta}^{ML}$, we find the estimator $\hat{Q}^{ML}$ of the underlying distribution. In other words, we will simply use the MLE instead of $\theta$ in the given formula: $\hat{Q}^{ML} = \mathcal{N}(\hat \theta^{ML}, 1) = \mathcal{N}(\bar X_n, 1)$.

## Part b
In practice, it is possible that $Q$ is not in the proposed model. For example, analysts used a normal distribution to model stock returns for decades. However, financial crises showed that actual stock returns are negatively skewed (more negative shocks than positive shocks) and have fat tails. A similar situation might be true with $Q$. We might believe that the underlying distribution is normal; however, the skewness and kurtosis of the distribution can be very different from a Gaussian model's respective moments. 

If this is the case, $\hat{Q}^{ML}$ might be a biased estimator of $Q$. In the class, we saw that MLE can produce very different results when one assumes that a Laplace distribution generated the underlying data instead of a normal distribution. Similarly, if the underlying distribution is not normal, $\hat{Q}^{ML}$, which is based on $\hat{\theta}^{ML}$, can produce very unreliable results.

## Part c
$$Q=0.99P_0+0.01P_{300}$$
The expected value or mean of $Q$ can be computed as a convex combination component means.
$$\mathbb{E}Q = \mathbb{E}\left[0.99P_0+0.01P_{300} \right] = 0.99\mathbb{E}(P_0) + 0.01\mathbb{E}(P_{300}) = 0.99 \times 0+0.01 \times 300=3$$
The density function of $Q$ is defined as a convex combination of component pdfs.
\begin{align*}
    f_X(x) &= 0.99 \times \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (X - 0)^{2}} + 0.01 \times \frac{1} {\sqrt{2\pi}} e^{-\frac{1}{2} (X - 300)^{2}}\\
    &= \frac{1}{\sqrt{2\pi}} \left(0.99 \times e^{-\frac{1}{2} X^{2}} + 0.01 \times e^{-\frac{1}{2} (X - 300)^{2}} \right)
\end{align*}
Clearly, this function cannot be simplified further to arrive at a density function of the normal distribution with $\theta = 3$ and $\sigma^2 = 1$. Therefore, we conclude that $Q$ does not belong to $\{\mathcal{N}(\theta, 1) : \theta \in \mathbb{R}\}$

## Part d
$\hat{Q}^{ML}$ is not close to $Q$ because of the following reasons:

  1. $\hat{Q}^{ML}$ is based on $\hat{\theta}^{ML}$, and the MLE (which is the empirical mean) is very sensitive to outliers. We know that $1\%$ of observations are generated by $\mathcal{N}(300, 1)$. As a result, the empirical mean will be pulled to the right significantly by these very large observations.
  2. $98.86\% (= 0.99 \times 99.86\%$) of the underlying observations lie within 3 standard deviations from $0$, that is, on the interval $[-3, 3]$. However, if we compute the MLE for the mean using a sample having outliers, we would discover that $\hat{\theta}^{ML}$ is around $3$, and the mean of $\hat{Q}^{ML}$ (the $50$th percentile) lies around the $98.86$th percentile of $Q$.

## Part e
To have a robust estimator of the mean, one can consider the following 3 options: 1) sample median; 2) trimmed mean; and 3) Winsorized mean. The problem with the sample median is that it uses only 1 observation when $n$ is odd and 2 observations when $n$ is even. As a result, we would not be able to get as much information from the sample as it can offer. However, we can exploit the fact the mean of $\hat{Q}$ is also its $50$th percentile. Therefore, we can choose such a quantile from $\mathcal{N}(0, 1)$ that would also act as the $50$th percentile of $Q$. The value of this estimate, $q$, would be derived using the cumulative distribution.
\begin{align*}
    F_Q(q) &= \int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} \left(0.99 \times e^{-\frac{1}{2} x^{2}} + 0.01 \times e^{-\frac{1}{2} (x - 300)^{2}} \right) dx\\
    &= 0.99 \times \int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^{2}}dx + 0.01 \times \int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (x-300)^{2}} dx\\
    & \approx 0.99 \times \int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^{2}}dx
\end{align*}
The second term is negligible and can be ignored because we know that $q$ should be in the interval where the bulk of the data lie: $[-3, 3]$. To find the $50$th percentile of $Q$, we equate its cdf to $0.5$ and use the quantile function of the distribution $\mathcal{N}(0, 1)$.
$$0.99 \times \int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^{2}}dx = \frac{1}{2}$$
$$\int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^{2}}dx = \frac{1}{2 \times 0.99}$$
$$\Phi(q) = \frac{1}{2 \times 0.99}$$
$$q = \Phi^{-1} \left(\frac{1}{2 \times 0.99} \right) \approx 0.013$$
Finally, the empirical median of a continuous distribution is strongly consistent, i.e, $\hat q \xrightarrow{a.s.} q$. Therefore, when $n$ is large enough, $\hat{Q}$ can be predicted fairly accurately using $\mathcal{N}(\hat q, 1)$. Also, $\hat Q$ will be very close to the bulk of the distribution, $\mathcal{N}(0, 1)$, and the estimator will not be affected by outliers. 

Two other alternatives (the Winsorized mean and trimmed mean) are usually close when we use the same percentage of replaced/removed observations. However, they suffer from the same problem: The choice of how many smallest and largest observations to replace/remove is arbitrary. If we imagine that we have a random sample of 100 observations from $Q$, we know that on average $1$ observation is generated by $\mathcal{N}(300, 1)$. Therefore, if we choose to replace/remove $1\%$ (or $2\%$ to be safe) of top and bottom observations, there is still $26.4\%$ ($7.9\%$) chance that the new sample will still contains an outlier. Thus, one should replace/remove the largest $3\%$ and the smallest $3\%$ of observations to reduce the probability of having an outlier to $1.8\%$ and make the Winsorized mean/trimmed mean fairly robust.

# 3 Hypothesis testing and doping controls
## Part a
Since the two players have different true hematocrit levels, we will write a statistical model for each one separately.
$$\{ \mathcal N(\tau_{JC}, \sigma_{JC}^2):(\tau_{JC}, \sigma_{JC}) \in \mathbb R_+ \times \mathbb R_+ \}$$
$$\{ \mathcal N(\tau_{SR}, \sigma_{SR}^2):(\tau_{SR}, \sigma_{SR}) \in \mathbb R_+ \times \mathbb R_+ \}$$

## Part b
By default, we assume that an athlete follows the requirements of a sport competition. Therefore, our null and alternative hypotheses are formulated as follows.
$$H_0: \tau_{JC} = 45 \qquad\text{vs}\qquad  H_1: \tau_{JC} > 45$$
$$H_0: \tau_{SR} = 45 \qquad\text{vs}\qquad  H_1: \tau_{SR} > 45$$

## Part c
Since the underlying distribution is Gaussian, we will use so-called Z-statistic. Under $H_0$, this Z-statistic follows the standard normal distribution, $\mathcal{N}(0, 1)$.
$$Z = \frac{X - \tau_0} {\sigma_0} = \frac{X - 45} {2}$$

## Part d
Since the alternative hypotheses are $\tau_{JC} > 45$ and $\tau_{SR} > 45$, we should choose a critical $\tau_c$ (or $z_c$) depending on the given probability of the Type-I error, $\alpha$. If the observed the hematocrit level exceeds this $\tau_c$, this will support the claim that a player is guilty. Therefore, the rejection region is the right tail of the distribution $\mathcal{N}(45, 4)$ ($\mathcal{N}(0, 1)$) at or above $\tau_c$ ($z_c$).

## Part e
### i
If $\tau_c = 45$ or $z_c = \frac{45 - 45} {2} = 0$, the probability of wrongly concluding that an athlete is doped is $50\%$.
$$P(X > 45) = P(Z > 0) = 1 - P(Z \leq 0) = 1 -  0.5 = 0.5$$

### ii
If $\tau_c = 60$ or $z_c = \frac{60 - 45} {2} = 7.5$, the probability of wrongly concluding that an athlete is doped is $0\%$.
$$P(X > 60) = P(Z > 7.5) = 1 - P(Z \leq 7.5) \approx 1 -  1 = 0$$

### iii
We would like to find $\tau_c$ for which the probability of the Type-I error, $\alpha$, is $0.05$
$$P(X > \tau_c) = P \left(Z > \frac{\tau_c - 45}{2} \right) = 1 - P \left(Z \leq \frac{\tau_c - 45}{2} \right) = 0.05$$
$$P \left(Z \leq \frac{\tau_c - 45}{2} \right) = 0.95$$
$$\Phi \left(\frac{\tau_c - 45}{2} \right) = 0.95$$
$$\frac{\tau_c - 45}{2} = \Phi^{-1} \left(0.95 \right) \approx 1.645$$
$$\tau_c = 1.645 \times 2 + 45 = 48.29$$

## Part f
### i and ii
We would reject $H_0$ and conclude that a player is doped whenever $X > \tau_c$ for the given $\alpha$. We know that $X_{JC} = 48$ and $X_{SR} = 50$ and that $\tau_c$ is $45$, $60$, and $48.29$ under the rules i, ii, and iii, respectively. Under rule i, we reject $H_0$ for both players. Under rule ii, we accept $H_0$ for both players. Under rule iii, we accept $H_0$ for J.C. and reject it for S.R.

### iii
$$P \left(X > 48.29 \right) = P \left(Z > \frac{48.29 - 45}{2} \right) = P(Z>1.645) = 1 - P(Z \leq 1.645) \approx 1 -  0.95 = 0.05$$
So, if $\tau_c = 48.29$ and $H_0$ is true, we would wrongly reject it with an average frequency of $0.05$. Therefore, if $100$ innocent athletes take the test with this rejection rule, we expect to find $5$ of them doped on average.

## Part g
### i and ii
$$\beta(\tau) = P_\tau(X > \tau_c) = P_\tau \left(Z > \frac{\tau_c - \tau}{2} \right) = 1- P_\tau \left(Z \leq \frac{\tau_c - \tau}{2} \right) = 1- \Phi_\tau \left(\frac{\tau_c - \tau}{2} \right)$$
Here, $\tau_c$ is $45$, $60$, and $48.29$ under the rules i, ii, and iii, respectively.
```{r}
tao_cr <- c(45, 60, 48.29)
taos <- seq(0, 100, by = 0.25)
for (tao in tao_cr) {
  z_scores <- (tao - taos) / 2
  power_func <- 1 - pnorm(z_scores, 0, 1, TRUE)
  plot(taos, power_func, type = 'l', col = 'blue', lwd = 2, xlab = "True Mean",
       ylab = "Power Function", main = paste("Rejection region starting from", tao))
  print(paste("The probability of rejection when 50:", 
              format(power_func[taos == 50], digits = 4)))
}
```

# 4 Data analysis
## Part a
```{r, message=FALSE}
rm(list = ls())
mydata <- read_csv("poulpeF.csv")
summary(mydata)
```
We see from looking at the summary that the lowest octopus weight represented is 40 and the highest is 2400. The median (545) is lower than the mean (639), suggesting that the distribution of weights is right-skewed.

## Part b
We saw in the class that the MLE of the mean and variance of a normal distribution are the empirical mean and empirical variance of data. In R, these can be conveniently calculated using `mean` and `var` functions. Since the formula for the built-in `var` function uses $n-1$ in its denominator, we will just scale it appropriately.
```{r}
n <- length(mydata$Poids)
mle_mean <- mean(mydata$Poids)
mle_var <- var(mydata$Poids) * (n-1)/n
print(c("MLE of the mean" = mle_mean, "     MLE of the variance" = mle_var))
```

## Part c
```{r}
hist(mydata$Poids, main = "Distribution of Data", xlab = "Weigths")
```
From the graph, it is clear that the distribution of weights does not follow a Gaussian model. Using a Gaussian model for hypothesis testing or interval estimation would be a problem because of two reasons:

  1. The distribution is heavily skewed to the right.
  2. The distribution is bounded below by zero, and the proportion of observations in the leftmost bin is not small.

Therefore, a Gaussian model does not seem appropriate for the above distribution. 

## Part d
We will use $\bar X_n$ for the empirical mean and $\hat \sigma$ for the empirical variance, both of which are MLE for a normal distribution. From the Central Limit Theorem, we know the following:
$$\frac {\sqrt{n} \left(\bar X_n - \mu\right)}{\sigma} \xrightarrow{D} \mathcal{N}(0, 1)$$
In addition, a normal distribution satisfies the regularity conditions covered in the class. Therefore, the MLE estimator of the population variance, $\hat \sigma$, is consistent estimator of $\sigma$. Said otherwise, $\hat \sigma \xrightarrow{P} \sigma$. Using the Continuous Map Theorem several times as well as the Slutsky's Theorem, we will arrive at the following result.
$$\frac {\sqrt{n} \left(\bar X_n - \mu\right)}{\sigma} \frac{\sigma}{\hat \sigma} \xrightarrow{D} \mathcal{N}(0, 1) \times 1$$
$$\frac {\sqrt{n} \left(\bar X_n - \mu\right)}{\hat \sigma} \xrightarrow{D} \mathcal{N}(0, 1)$$
Putting it all together, we will arrive at a confidence interval for $\mu$:
$$\bar X_n \pm z_{1 - \alpha / 2} \times \frac{\hat \sigma}{\sqrt{n}}$$
```{r}
alpha <- 0.05
z_score <- qnorm(1 - alpha/2, 0, 1, TRUE)
lower <- mle_mean - z_score * sqrt(mle_var) / sqrt(n)
upper <- mle_mean + z_score * sqrt(mle_var) / sqrt(n)
print(c("Lower bound" = lower, "   Upper bound" = upper))
```