---
title: "Statistical Inference: Homework 2"
author: "Asrorbek Orzikulov, Sofya Simakova"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, message=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.align = 'center', echo = TRUE)
```

# 1 Comparison of estimators

## Part a
We will first write a likelihood function for $X \sim {Bin}(n, \theta)$.
$$\mathcal{L} \left(\theta, X\right)= P_{X}\left(x\right)=\left(\begin{array}{c}
n\\
X
\end{array}\right) \theta^{X}(1-\theta)^{(n-X)}$$
$$\ell \left(\theta, X \right)=\log \mathcal{L} \left(\theta, X\right)=\log \left(\begin{array}{c}
n\\
X
\end{array}\right)
+ X \log \theta+\left(n-X\right) \log (1-\theta)$$
$$\frac{\partial \ell \left(\theta, X\right)}{\partial \theta}=\frac{X}{\theta}-\frac{n-X}{1-\theta}=0$$
$$\frac{X}{\theta}= \frac{ n-X}{1-\theta}$$
$${X}-{X} \theta= n \theta -{X} \theta$$
$${X}=\theta n$$
$$\boxed{\hat{\theta}^{ML}=\frac{X}{n}}$$
To check if this value is indeed maximum, we should consider the second derivative of the log-likelihood function.
$$\frac{\partial^2 \ell \left(\theta, X\right)}{\partial \theta^2} = - \frac{X}{\theta^2} - \frac{n-X}{\left(1-\theta\right)^2} < 0$$
Also, we should demonstrate that the optimum is not at the boundaries of the interval for $\theta$ -- $[0, 1]$. Since $\ell \left(\theta, X\right)$ depends on $\log \theta$ and $\log (1 - \theta)$, the value of the log-likelihood function when $\theta$ is either $0$ or $1$ is $-\infty$. Therefore, we can conclude that $\hat{\theta}^{ML}$ is a Maximum Likelihood Estimator.

From the fact that the mean of a binomial distribution is $n\theta$ and its variance is $n\theta(1-\theta)$ we have:
$$\mathbb E \; \hat{\theta}^{ML}= \mathbb E \left[\frac{X}{n}\right] =\frac{1}{n} \mathbb EX =\frac{1}{n} n \theta = \theta$$
$$Var\:\hat{\theta}^{ML}= Var\left[\frac{X}{n}\right]=\frac{1}{n^2} Var X =\frac{1}{n^2} n\theta(1-\theta)=\frac{\theta(1-\theta)}{n}$$

## Part b
$$\hat{\theta}^{alt}=\frac{X+1}{n+2}$$
$$\mathbb E \:\hat{\theta}^{alt} = \mathbb E\left[\frac{X+1}{n+2}\right] = \frac{1}{n+2} \mathbb E[X+1]=\frac{1}{n+2}[n \theta+1]=\frac{n \theta+1}{n+2}$$
$$Var\:\hat{\theta}^{alt}=Var\left[\frac{X+1}{n+2}\right]=\frac{1}{(n+2)^{2}} \operatorname{Var}[X+1]=\frac{\operatorname{Var}[X]}{(n+2)^{2}}=\frac{n \theta(1-\theta)}{(n+2)^{2}}$$

## Part c
$$\operatorname{Bias} {\hat{\theta}^{ML}} = \mathbb E \hat{\theta}^{ML} - \theta = \theta - \theta = 0$$
$$Var \; {\hat{\theta}^{ML}} = \frac{ \theta(1-\theta)}{n}$$
$$\operatorname{MSE} \; \hat{\theta}^{ML} = \left( \operatorname{Bias} {\hat{\theta}^{ML}} \right)^2 + Var \; {\hat{\theta}^{ML}} = 0^2 + \frac{\theta(1-\theta)}{n} = \frac{ \theta(1-\theta)}{n}$$

$$\operatorname{Bias} {\hat{\theta}^{alt}} = \mathbb E \hat{\theta}^{alt} - \theta = \frac{n \theta+1} {n+2} -\theta = \frac{n\theta+1-n \theta - 2 \theta}{n+2} = \frac{1 - 2 \theta}{n+2}$$
$$Var\:\hat{\theta}^{alt} = \frac{n \theta(1-\theta)}{(n+2)^{2}}$$
\begin{align*}
\operatorname{MSE} \; \hat{\theta}^{alt} &= \left( \operatorname{Bias} {\hat{\theta}^{alt}} \right)^2 + Var \; {\hat{\theta}^{alt}} = \left(\frac{1 - 2 \theta}{n+2} \right)^2 + \frac{n \theta(1-\theta)} {(n+2)^{2}}\\
&= \frac{(1-2 \theta)^{2} + n \theta(1-\theta)}{(n+2)^{2}} = \frac{1-4 \theta+4 \theta^{2}+n \theta-n \theta^{2}}{(n+2)^{2}}\\
&= \frac{1+(n-4) \theta-(n-4) \theta^{2}}{(n+2)^{2}}
\end{align*}

Comparison of the two estimators leads us to 4 immediate conclusions:

  1. The MLE is unbiased, while the alternative estimator is biased whenever $\theta \neq 0.5$.
  2. Since $\frac{1}{n} > \frac{n}{(n+2)^2}$, the variance of the alternative estimator is smaller than that of the MLE. As a result, we can observe so-called bias-variance tradeoff in this question.
  3. As $n\to\infty$, the bias of $\hat{\theta}^{alt}$ goes to zero and the difference between  $\frac{1}{n}$ and $\frac{n}{(n+2)^2}$ diminishes. Therefore, when $n$ is large, these two estimators are almost identical.
  4. If one wants to carry out a statistical analysis (e.g. conduct hypothesis testing or construct a confidence interval), they should choose the MLE since it is unbiased. However, we will see in **part d** that the MSE of $\hat{\theta}^{alt}$ is lower than that of $\hat{\theta}^{ML}$ when $\theta \in [0.15, 0.85]$. Over this range, data scientists would prefer $\hat{\theta}^{alt}$ to $\hat{\theta}^{ML}$ because for any single instance of true $\theta$, the alternative estimate will be closer to the actual value.

## Part d
For this question, we will use $n = 50$ and $\theta = 0.73$.
```{r}
rm(list = ls())
n <- 50
theta <- 0.73
X <- rbinom(n = 1, size = n, prob = theta)
X
```

```{r}
theta_mle <- X / n
bias_mle <- 0
var_mle <- theta * (1-theta) / n
mse_mle <- bias_mle^2 + var_mle
print(c("theta_mle" = theta_mle, "bias_mle" = bias_mle, 
        "var_mle" = var_mle, "mse_mle" = mse_mle))

theta_alt <- (X+1) / (n+2)
bias_alt <- (1 - 2*theta) / (n+2)
var_alt <- (n*theta*(1-theta)) / (n+2)^2
mse_alt <- bias_alt^2 + var_alt
print(c("theta_alt" = theta_alt, "bias_alt" = bias_alt, 
        "var_alt" = var_alt, "mse_alt" = mse_alt))
```

For this question, we will use $n = 50$ and show the MSE of $\hat \theta^{MLE}$ and $\hat \theta^{alt}$ as a function of $\theta \in [0, 1]$.
```{r}
thetas <- seq(0, 1, by = 0.005)
mse_mle <- c()
mse_alt <- c()
for (theta in thetas) {
  mse_mle <- c(mse_mle, theta * (1-theta) / n)
  mse_alt <- c(mse_alt, (1 + (n-4)*theta - (n-4)*theta^2) / (n+2)^2)
}
plot(thetas, mse_mle, type = 'l', col = 'red', lwd = 2, ylab = "MSE")
points(thetas, mse_alt, type = 'l', col = 'blue', lwd = 2, ylab = "MSE")
legend("topleft", legend = c("MLE", "alternative"),
       col = c("red", "blue"), lty = 1, cex = 1)
```

# 2 Robustness of the estimators
## Part a
The maximum likelihood function for a Gaussian model whose variance is 1 is given below.
$$\mathcal {L}_n(\theta,X_1, \ldots, X_n) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (X_i -\theta)^{2}}$$
To compute the Maximum Likelihood estimator $\hat{\theta}^{ML}$, we find the zero of the first derivative of the log-likelihood function.
\begin{align*}
\ell_n (\theta,X_1, \ldots , X_n) &= \log \mathcal {L}_n(\theta,X_1, \ldots, X_n)= \log \left(\prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} (X_i -\theta)^{2}} \right)\\
&= \sum_{i=1}^{n}\log \left(\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} (X_i -\theta)^{2}} \right) = \sum_{i=1}^{n} \left(\log \frac{1}{\sqrt{2\pi}} + \log  e^{-\frac{1}{2} (X_i -\theta)^{2}} \right)\\
&= \sum_{i=1}^{n} \left(\log \frac{1}{\sqrt{2\pi}} -\frac{1}{2} (X_i -\theta)^{2} \right) = n \log \frac{1}{\sqrt{2\pi}} -\frac{1}{2} \sum_{i=1}^{n} \left(X_i -\theta \right)^{2}
\end{align*}
$$\frac{\partial \ell_n (\theta,X_1, \ldots , X_n)}{\partial \theta} = -\frac{1}{2} \sum_{i=1}^{n} 2(X_i - \theta) (-1) = 0$$
$$\frac{\partial \ell_n (\theta,X_1, \ldots , X_n)}{\partial \theta} = \sum_{i=1}^{n} (X_i - \theta) = 0$$
$$\sum_{i=1}^{n}(X_i - \theta) = 0$$
$$\sum_{i=1}^{n} X_i - \sum_{i=1}^{n} \theta = 0$$
$$n\theta= \sum_{i=1}^{n} X_i$$
$$\boxed{\hat{\theta}^{ML} = \bar{X}_n}$$
To prove that this estimator is indeed maximum, we will demonstrate that the second derivative is negative.
$$\frac{\partial^2 \ell_n (\theta,X_1, \ldots , X_n)}{\partial \theta^2} = \sum_{i=1}^{n} (- 1) = -n < 0$$
Using this $\hat{\theta}^{ML}$, we find the estimator $\hat{Q}^{ML}$ of the underlying distribution. In other words, we will simply use the MLE instead of $\theta$ in the given formula: $\hat{Q}^{ML} = \mathcal{N}(\hat \theta^{ML}, 1) = \mathcal{N}(\bar X_n, 1)$.

## Part b
In practice, it is possible that $Q$ is not in the proposed model. For example, analysts used a normal distribution to model stock returns for decades. However, financial crises showed that actual stock returns are negatively skewed (more negative shocks than positive shocks) and have fat tails. A similar situation might be true with $Q$. We might believe that the underlying distribution is normal; however, the skewness and kurtosis of the distribution can be very different from a Gaussian model's respective moments. 

If this is the case, $\hat{Q}^{ML}$ might be a biased estimator of $Q$. In the class, we saw that MLE can produce very different results when one assumes that a Laplace distribution generated the underlying data instead of a normal distribution. Similarly, if the underlying distribution is not normal, $\hat{Q}^{ML}$, which is based on $\hat{\theta}^{ML}$, can produce very unreliable results.

## Part c
$$Q=0.99P_0+0.01P_{300}$$
The expected value or mean of $Q$ can be computed as a convex combination component means.
$$\mathbb{E}Q = \mathbb{E}\left[0.99P_0+0.01P_{300} \right] = 0.99\mathbb{E}(P_0) + 0.01\mathbb{E}(P_{300}) = 0.99 \times 0+0.01 \times 300=3$$
The density function of $Q$ is defined as a convex combination of component pdfs.
\begin{align*}
    f_X(x) &= 0.99 \times \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (X - 0)^{2}} + 0.01 \times \frac{1} {\sqrt{2\pi}} e^{-\frac{1}{2} (X - 300)^{2}}\\
    &= \frac{1}{\sqrt{2\pi}} \left(0.99 \times e^{-\frac{1}{2} X^{2}} + 0.01 \times e^{-\frac{1}{2} (X - 300)^{2}} \right)
\end{align*}
Clearly, this function cannot be simplified further to arrive at a density function of the normal distribution with $\theta = 3$ and $\sigma^2 = 1$. Therefore, we conclude that $Q$ does not belong to $\{\mathcal{N}(\theta, 1) : \theta \in \mathbb{R}\}$

## Part d
$\hat{Q}^{ML}$ is not close to $Q$ because of the following reasons:

  1. $\hat{Q}^{ML}$ is based on $\hat{\theta}^{ML}$, and the MLE (which is the empirical mean) is very sensitive to outliers. We know that $1\%$ of observations are generated by $\mathcal{N}(300, 1)$. As a result, the empirical mean will be pulled to the right significantly by these very large observations.
  2. $98.86\% (= 0.99 \times 99.86\%$) of the underlying observations lie within 3 standard deviations from $0$, that is, on the interval $[-3, 3]$. However, if we compute the MLE for the mean using a sample having outliers, we would discover that $\hat{\theta}^{ML}$ is around $3$, and the mean of $\hat{Q}^{ML}$ (the $50$th percentile) lies around the $98.86$th percentile of $Q$.

## Part e
Imagine that we have a random sample of 100 observations drawn from the distribution $Q$. To have a robust estimator of the mean, one can consider the following 3 options: 1) sample median; 2) trimmed mean; and 3) Winsorized mean. The problem with the sample median is that it uses only 1 observation when $n$ is odd and 2 observations when $n$ is even. As a result, we would not be able to get as much information from the sample at hand as it can offer. The Winsorized mean and trimmed mean are usually close when we use the same percentage of replaced/removed observations. However, they suffer from the same problem: The choice of how many smallest and largest observations to replace/remove is arbitrary. In the above case, we know that on average $1\%$ of observations are generated by $\mathcal{N}(300, 1)$. Therefore, if we choose to replace/remove $1\%$ (or $2\%$ to be safe) of top and bottom observations, there is still $26.4\%$ ($7.9\%$) chance that the new sample will still contains an outlier. Thus, one should replace/remove the largest $3\%$ and the smallest $3\%$ of observations to reduce the probability of having an outlier to $1.8\%$ and make the Winsorized mean/trimmed mean fairly robust.

Alternatively, we can exploit the fact the mean of $\hat{Q}$ is also its $50$th percentile. Therefore, we can choose such a percentile from $\mathcal{N}(0, 1)$ that would also act as the $50$th percentile of $Q$. The value of this estimate, $q$, would be derived using the cumulative distribution.
\begin{align*}
    F_Q(q) &= \int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} \left(0.99 \times e^{-\frac{1}{2} x^{2}} + 0.01 \times e^{-\frac{1}{2} (x - 300)^{2}} \right) dx\\
    &= 0.99 \times \int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^{2}}dx + 0.01 \times \int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} (x-300)^{2}} dx\\
    & \approx 0.99 \times \int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^{2}}dx
\end{align*}
The second term is negligible and can be ignored because we know that $q$ should be in the interval where the bulk of the data lie: $[-3, 3]$. To find the $50$th percentile of $Q$, we equate its cdf to $0.5$ and use the quantile function of the distribution $\mathcal{N}(0, 1)$.
$$0.99 \times \int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^{2}}dx = \frac{1}{2}$$
$$\int_{-\infty}^{q} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^{2}}dx = \frac{1}{2 \times 0.99}$$
$$\Phi(q) = \frac{1}{2 \times 0.99}$$
$$q = \Phi^{-1} \left(\frac{1}{2 \times 0.99} \right) \approx 0.013$$
Finally, $\hat{Q}$ would be predicted to be $\mathcal{N}(q, 1) = \mathcal{N}(0.013, 1)$.

# 4 Data analysis
## Part a
```{r, message=FALSE}
rm(list = ls())
mydata <- read_csv("poulpeF.csv")
summary(mydata)
```
We see from looking at the summary that the lowest octopus weight represented is 40 and the highest is 2400. The median (545) is lower than the mean (639), suggesting that the distribution of weights is right-skewed. 

## Part b
We saw in the class that the MLE of the mean and variance of a normal distribution are the empirical mean and empirical variance of data. In R, these can be conveniently calculated using `mean` and `var` functions. Since the formula for the built-in `var` function uses $n-1$ in its denominator, we will just scale it appropriately.
```{r}
n <- length(mydata$Poids)
mle_mean <- mean(mydata$Poids)
mle_var <- var(mydata$Poids) * (n-1)/n
print(c("MLE of the mean" = mle_mean, "     MLE of the variance" = mle_var))
```

## Part c
```{r}
hist(mydata$Poids, main = "Distribution of Data", xlab = "Weigths")
```
From the graph, it is clear that the distribution of weights does not follow a Gaussian model. Using a Gaussian model for hypothesis testing or interval estimation would be a problem because of two reasons:

  1. The distribution is heavily skewed to the right.
  2. The distribution is bounded below by zero, and the proportion of observations in the leftmost bin is not small.

Therefore, a Gaussian model does not seem appropriate for the above distribution. 

# Part d
We will use $\bar X_n$ for the empirical mean and $\hat \sigma$ for the empirical variance, both of which are MLE for a normal distribution. From the Central Limit Theorem, we know the following:
$$\frac {\sqrt{n} \left(\bar X_n - \mu\right)}{\sigma} \xrightarrow{D} \mathcal{N}(0, 1)$$
In addition, a normal distribution satisfies the regularity conditions covered in the class. Therefore, the MLE estimator of the population variance, $\hat \sigma$, is consistent estimator of $\sigma$. Said otherwise, $\hat \sigma \xrightarrow{P} \sigma$. Using the Continuous Map Theorem several times as well as the Slutsky's Theorem, we will arrive at the following result.
$$\frac {\sqrt{n} \left(\bar X_n - \mu\right)}{\sigma} \frac{\sigma}{\hat \sigma} \xrightarrow{D} \mathcal{N}(0, 1) \times 1$$
$$\frac {\sqrt{n} \left(\bar X_n - \mu\right)}{\hat \sigma} \xrightarrow{D} \mathcal{N}(0, 1)$$
Putting it all together, we will arrive at a confidence interval for $\mu$:
$$\bar X_n \pm z_{1 - \alpha / 2} \times \frac{\hat \sigma}{\sqrt{n}}$$
```{r}
alpha <- 0.05
z_score <- qnorm(1 - alpha/2, 0, 1, TRUE)
lower <- mle_mean - z_score * sqrt(mle_var) / sqrt(n)
upper <- mle_mean + z_score * sqrt(mle_var) / sqrt(n)
paste("Lower bound of the confidence interval is", lower)
paste("Upper bound of the confidence interval is", upper)
```