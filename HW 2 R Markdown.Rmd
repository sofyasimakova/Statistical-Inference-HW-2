---
title: "HW 2"
author: "Asrorbek Orzikulov, Sofya Simakova"
date: "3/27/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 Comparison of estimators (5pts)

$X \sim {Bin}(n, \theta)$

## a.
$\mathcal{L}_{n}\left(\theta, X\right) = f\left(\theta, X\right)$

For one random sample, we have the following:

$\mathcal{L}_n\left(\theta, X\right)= P_{X}\left(x\right)=\left(\begin{array}{l}
n \\
x
\end{array}\right) \theta^{x}(1-\theta)^{(n-x)}$

$\ell_n \left(\theta, X \right)=\log \mathcal{L}_{n}\left(\theta, X\right)=\log \left(\begin{array}{l}
n \\
X
\end{array}\right)
+ X \log \theta+\left(n-X\right) \log (1-\theta)$

$\frac{\partial \ell_n \left(\theta, X\right)}{\partial \theta}=\frac{X}{\theta}-\left(\frac{n-X}{1-\theta}\right)=0$

$\frac{X}{\theta}=\left(\frac{ n-X}{1-\theta}\right)$

Dividing both sides by N, we have:

$\frac{\bar{X}}{\theta}=\frac{n-\bar{X}}{1-\theta}$

$\bar{X}-\bar{X} \theta=\theta n-\bar{X} \theta$

$\bar{X}=\theta n$

$\hat{\theta}^{ML}=\frac{\bar{X}}{n}=\frac{X}{n}$ (estimate)

$\boxed{\hat{\theta}^{ML}=\frac{X}{n}}$ - MLE

$E\:\hat{\theta}^{ML}=E[\left(\frac{X}{n}\right]=\frac{1}{n} E\left[X\right]=\frac{1}{n} EX
=\frac{1}{n} n \theta=\theta$

From the fact that the mean of a binomial distribution is $n\theta$ and its variance is $n\theta(1-\theta)$:

Var$\:\hat{\theta}^{ML}= Var\left[\frac{X}{n}\right]=\frac{1}{n^2}Var\left[{X}\right]\stackrel{i.i.d}{=}
\frac{1}{n^2} Var X =\frac{1}{n^2} Var X =\frac{1}{n^2} n\theta(1-\theta)=\frac{\theta(1-\theta)}{n}$

## b.
$\hat{\theta}^{Alt.}=\frac{(X+1)}{(n+2)}$

$\begin{aligned}
& E \:\hat{\theta}^{Alt.}=E\left[\frac{X+1}{n+2}\right]=\frac{1}{n+2} E[X+1]=\frac{1}{n+2}[n \theta+1]=\frac{n \theta+1}{n+2}
\end{aligned}$

$Var\:\hat{\theta}^{Alt.}=Var\left[\frac{X+1}{n+2}\right]=\frac{1}{(n+2)^{2}} \operatorname{Var}[X+1]=\frac{\operatorname{Var}[X]}{(n+2)^{2}}=\frac{n \theta(1-\theta)}{(n+2)^{2}}$

## c.
$\operatorname{MSE}\left(\hat{\theta}^{ML}\right)=\left(E\hat{\theta}^{ML}-\theta\right)^{2}+\operatorname{Var} \hat{\theta}^{ML}=(\theta-\theta)^{2}+\frac{\theta(1-\theta)}{n}=\frac{ \theta(1-\theta)}{n}$

$\operatorname{MSE}\left(\hat{\theta}^{Alt}\right)=\left(E \hat{\theta}^{Alt}-\theta\right)^{2}+\operatorname{Var} \hat{\theta}^{Alt}=\left(\frac{n \theta+1}{n+2}-\theta\right)^{2}+\frac{n \theta(1-\theta)}{(n+2)^{2}}=\left(\frac{n\theta+1-n \theta-2 \theta}{n+2}\right)^{2} +\frac{n \theta(1-\theta)}{(n+2)^{2}}=\frac{(1-2 \theta)^{2}+n \theta(1-\theta)}{(n+2)^{2}}=\frac{1-4 \theta+4 \theta^{2}+n \theta-n \theta^{2}}{(n+2)^{2}}= \frac{1+(n-4) \theta-(n-4) \theta^{2}}{(n+2)^{2}}$

## d.
Simulate a sample from a binomial distribution for some fixed $\theta$ and n.

```{r}
rm(list = ls())
n <- 50
theta <- 0.73
X <- rbinom(n = 1, size = n, prob = theta)
X
```

Calculate the two estimators, their bias and their variance
```{r}
theta_mle <- X / n
bias_mle <- 0
var_mle <- theta * (1-theta) / n
mse_mle <- bias_mle^2 + var_mle

theta_alt <- (X+1) / (n+2)
bias_alt <- (n*theta + 1) / (n+2) - theta
var_alt <- (n*theta*(1-theta)) / (n+2)^2
mse_alt <- bias_alt^2 + var_alt

paste("theta_mle is", theta_mle)
paste("bias_mle is", bias_mle)
paste("var_mle is", var_mle)
paste("mse_mle is", mse_mle)

paste("theta_alt is", theta_alt)
paste("bias_alt is", bias_alt)
paste("var_alt is", var_alt)
paste("mse_alt is", mse_alt)
```

Represent (on the same graph), the MSE of $\theta^{MLE}$ and $\theta^{alt.}$ as a function of
$\theta \in$	 [0; 1] for fixed n.
```{r}
thetas <- seq(0, 1, by = 0.005)
mse_mle <- c()
mse_alt <- c()
for (theta in thetas) {
  mse_mle <- c(mse_mle, theta * (1-theta) / n)
  mse_alt <- c(mse_alt, (1 + (n-4)*theta - (n-4)*theta^2) / (n+2)^2)
}

plot(thetas, mse_mle, type = 'l', col = 'red', lwd = 2, ylab = "MSE")
points(thetas, mse_alt, type = 'l', col = 'blue', lwd = 2, ylab = "MSE")

legend("topleft", legend = c("MLE", "Alt."),
       col = c("red", "blue"), lty = 1, cex = 1)
```

# 4 Data analysis (5pts).

## a.
```{r}
library(tidyverse)
mydata <- read_csv("/Users/user/Desktop/MSc DSBA /M1 - T2/Statistical Inference/Homework 2/poulpeF.csv")
summary(mydata)
```

We see from looking at the summary that the lowest octopus weight represented is 40 and the highest is 2400. The median (545) is lower than the mean (639) - meaning, distribution ow weights is right-skewed. 
## b.

Calculating MLE of the mean:

```{r}
sum <- 0
n <-0
for (x in mydata$Poids){
  sum <- sum + x
  n <- n+1
}
mle_mean = sum/n
paste("MLE of the mean is ", mle_mean)
```

Calculating MLE of the variance:

```{r}
sum <- 0
n <- 0
mean <- mle_mean #we know that for normal distribution, mle of mean is equal to sample mean

for (x in mydata$Poids){
  diff_squared <- (x - mean)^2
  sum <- sum + diff_squared
  n <- n+1
}
mle_var <- sum / n
paste("MLE of the variance is ", mle_var)
```

## c.
```{r}
hist(mydata$Poids)
```

From the graph it is clear that the distribution of weights does not follow the Gaussian model.
Is skewes heavily to the right. Using Gaussian model for calculations would be a problem, 
as it would not provide accurate results.

# d.
```{r}
variance <- mle_var # we can say that because this is a Gaussian distribution
lower <- mean - 1.95 * sqrt(variance) / sqrt(n)
paste("Lower bound of the confidence interval is ", lower)
```
```{r}
upper <- mean + 1.95 * sqrt(variance) / sqrt(n)
paste("Upper bound of the confidence interval is ", upper)
```